{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generating realisations__ \n",
    "---\n",
    "\n",
    "[![colab image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/astro-informatics/s2scat/blob/main/notebooks/synthesis.ipynb)\n",
    "\n",
    "This tutorial is a basic overview of how one may use the scattering covariances as a statistical generative model. There are a number of different applications which are presented in other notebooks, take a look!\n",
    "\n",
    "In the machine learning (ML) literature, a generative model is typically associated with some model $M_{\\lbrace w,b \\rbrace}(\\theta)$, with weights and biases $\\lbrace w, b \\rbrace$, which takes parameters $\\theta$ for which a new realisation (very often an image) may be generated \n",
    "\n",
    "$$f = M_{w,b}(\\theta)$$\n",
    "\n",
    "The difficulty with this approach is that the quality of $f$ is highly dependent on the optimality of $\\lbrace w, b \\rbrace$, which in turn is entirely dependent on the abundance of realistic training data. In many (often high dimensional) application domains, such training data does not exist, axiomatically limiting naive ML approaches.\n",
    "\n",
    "One may instead construct an expressive statistical representation from which, provided at least a single fiducial realisation, many realisations may be drawn. This concept is actually very familiar, particularly in Cosmology where it is typical to e.g. draw Gaussian realisations from a known power spectrum $f \\sim P(L)$. In this simplistic case the process of drawing a new realisation is self evident, however this generative model does not capture complex non-linear structural information.\n",
    "\n",
    "Here we will instead use the scattering covariances $\\mathcal{S}$ as our statistical representation. Given $\\mathcal{S}$ is non-linear, generating new realisations isn't quite so straightforward, in fact to do so we'll need to solve an optimisation problem\n",
    "\n",
    "$$ f = \\min_{\\hat{f}}\\Big [ \\mathcal{S}(\\hat{f}) - \\mathcal{T} \\Big]$$\n",
    "\n",
    "where $\\mathcal{T} = \\mathcal{S}(f^{\\text{in}})$ are the target covariances computed from the signal we are aiming to emulate $f^{\\text{in}}$. To solve this optimisation with gradient based methods we clearly need to be able to differentiate through $\\mathcal{S}$ which is a complex function involving Wigner transforms and non-linearities. \n",
    "\n",
    "As ``S2SCAT`` is a ``JAX`` package, we can readily access these gradients, so lets see exactly how this works! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the package\n",
    "\n",
    "Lets first import ``S2SCAT`` and some basic plotting functions. We'll also pick up ``pickle`` to load the targets which have been stored just to save you some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, pickle\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "import s2scat, s2fft\n",
    "from s2scat.core import synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up the target field we are aiming to emulate, and the hyperparameters of the scattering covariance representation we will work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 128                # Spherical harmonic bandlimit.\n",
    "N = 3                  # Azimuthal bandlimit (directionality).\n",
    "J_min = 0              # Minimum wavelet scale.\n",
    "reality = True         # Input signal is real.\n",
    "recursive = False      # Use the fully precompute transform.\n",
    "\n",
    "# Lets load in the spherical field we wish to emulate and its harmonic coefficients.\n",
    "f = np.load('data/WL_example_f_{}.npy'.format(L))\n",
    "flm = np.load('data/WL_example_flm_{}.npy'.format(L))\n",
    "\n",
    "# Also lets load in the target scattering covariances, generated from f and flm.\n",
    "file = 'data/targets_{}.pickle'.format(L)\n",
    "with open(file, 'rb') as handle:\n",
    "    targets, norm = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before calling the scattering transform you need to run configuration, which will generate any precomputed arrays and cache them. When running the recurisve transform this shouldn't take much memory at all. However, the fully precompute transform can be extremely memory hungry at L ~ 512 and above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = s2scat.configure(L, N, J_min, reality, recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a simple $\\ell_2$-loss function which just computes the mean squared distance between the scattering covariances computed at our current iterant and those of the target. In practice, any loss could be considered here, however we'll use the most straightforward scenario for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(glm):\n",
    "    predicts = s2scat.scatter(glm, L, N, J_min, reality, config, norm, recursive)\n",
    "    return synthesis.l2_covariance_loss(predicts, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an initial estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose a set of harmonic coefficients $g_{\\ell m}$ from which to start our optimisation. Strictly speaking, we should start from Gaussianly distributed random signal to ensure we form a macro-canonical model of our target field, and we will do precisely this. However, in practice it may be better to start from e.g. a Gaussian random field, generated from a fiducial power spectrum, as this may reduce the total number of iterations required for convergence. \n",
    "\n",
    "In any case, lets generate a starting signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standard deviation of the target field.\n",
    "sigma_bar = np.std(np.abs(flm)[flm!=0])\n",
    "\n",
    "# Generate Gaussian random harmonic coefficients with the correct variance.\n",
    "glm = np.random.randn(L, L) * sigma_bar + 1j*np.random.randn(L, L) * sigma_bar \n",
    "\n",
    "# Save the starting noise signal for posterity and plotting!\n",
    "glm_start = s2scat.operators.spherical.make_flm_full(glm, L)\n",
    "g_start = s2fft.inverse(glm_start, L, reality=reality, method=\"jax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimise the objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass all these components to ``optax``, which we have internally configured to use the adam optimizer to minimise the loss and return us a synthetic realisation which should approximate the target field statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimisation to generate a new realisation glm.\n",
    "glm, _ = synthesis.fit_optax(glm, loss_func, niters=400, learning_rate=1e-3)\n",
    "\n",
    "# Convert the synthetic harmonic coefficients into a pixel-space image.\n",
    "glm_end = s2scat.operators.spherical.make_flm_full(glm, L)\n",
    "g_end = s2fft.inverse(glm_end, L, reality=reality, method=\"jax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets check how our starting and ending realisations shape up against the target field!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [f, g_start, g_end]\n",
    "titles = [\"Target\", \"Initial\", \"Emulation\"]\n",
    "fig, axs = plt.subplots(1,3, figsize=(30,10))\n",
    "mx, mn = np.nanmax(f), np.nanmin(f)\n",
    "for i in range(3):\n",
    "    axs[i].imshow(fields[i], cmap=\"magma\", vmax=mx, vmin=mn)\n",
    "    axs[i].set_title(titles[i])\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('s2scat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ff6fc9eb7f710a5e498c8d33dd504161241529ee837967084a282879cc0fea1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
